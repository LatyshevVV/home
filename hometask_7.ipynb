{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.04, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'is', 'via', 'there', 'although', 'where', 'towards', 'five', 'most', 'about', 're', 'whoever', 'themselves', 'never', 'hereafter', 'this', 'last', 'call', 'indeed', 'much', 'few', 'them', 'yourself', 'full', 'thereupon', 'thru', 'herein', 'see', 'former', 'thin', 'already', 'f...ehow', 'now', 'his', 'has', 'how', 'if', 'either', 'inc', 'nothing', 'will', 'too', 'him', 'would'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS, analyzer='word', binary=True, min_df = 10, max_df =.04)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10297 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def lda(n, X, niter , a, b):\n",
    "    nkw = np.zeros((n, X.shape[1]))\n",
    "    ndk = np.zeros((X.shape[0], n))\n",
    "    nk = np.zeros(n)\n",
    "    docs, words = X.nonzero()\n",
    "    tags = np.random.choice(n, len(docs))\n",
    "    for w,d,t in zip(words, docs, tags):\n",
    "        nkw[t,w] += 1\n",
    "        ndk[d,t] += 1\n",
    "        nk[t] +=1\n",
    "    for i in tqdm(range(niter)):\n",
    "        for j in range(len(docs)):\n",
    "            t=tags[j]\n",
    "            nkw[t,words[j]] -= 1\n",
    "            ndk[docs[j], t] -=1\n",
    "            nk[t] -=1\n",
    "            p=(ndk[docs[j], :] + a)*(nkw[:,words[j]] + b[words[j]]) /(nk + b.sum())\n",
    "            tags[j] = np.random.choice(np.arange(n), p = p / p.sum())\n",
    "            nkw[tags[j], words[j]] += 1\n",
    "            ndk[ docs[j], tags[j]] += 1\n",
    "            nk[tags[j]] += 1\n",
    "    return nkw, ndk, nk, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [40:27<00:00, 48.43s/it]\n"
     ]
    }
   ],
   "source": [
    "n=20\n",
    "nkw, ndk, nk, tags= lda(n, X_train, 50, 1*np.ones(n), 1*np.ones(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\tanybody\tarticle\tcheers\tcurious\tdeleted\tknows\tposting\trecall\tsound\tstuff\n",
      "Topic 1:\t11\t12\t13\t14\t16\t17\t18\t19\t24\t93\n",
      "Topic 2:\tchicago\tgame\tgames\thockey\tplay\trangers\tst\tteam\ttoronto\twin\n",
      "Topic 3:\tbible\tchrist\tchristian\tchristians\tclaim\tjesus\tman\treligion\tsaying\tword\n",
      "Topic 4:\tarmenian\tarmenians\tchildren\thistory\tkilled\tmen\tsource\ttoday\tturkish\twar\n",
      "Topic 5:\tcame\tdays\thappened\thome\tleft\tsaw\tstarted\ttold\ttook\twent\n",
      "Topic 6:\tbanks\tcause\tdisease\teffect\tgordon\tmedical\tnormal\tpitt\tsoon\tsurrender\n",
      "Topic 7:\t14\t24\t50\tah\thp\tma\tmi\tmr\tms\ttm\n",
      "Topic 8:\tcare\tchange\tfeel\tguess\thand\thappen\thaven\tmoney\toh\tsort\n",
      "Topic 9:\tearth\tlarge\tlow\tnasa\tproject\tresearch\tscience\tsmall\tspace\tsystems\n",
      "Topic 10:\tagree\tanti\tarab\tarabs\tisrael\tisraeli\tjewish\tjews\tpeace\twar\n",
      "Topic 11:\tapplication\tcode\tdisplay\tfile\tfiles\tftp\trunning\tserver\tversion\twindow\n",
      "Topic 12:\taddress\tadvance\tappreciate\tcurrent\tfax\thi\tinternet\tsimple\tthank\tuniversity\n",
      "Topic 13:\tamerican\tclinton\tcontrol\tgun\thouse\tlaw\tnational\tpublic\trights\tstates\n",
      "Topic 14:\tbike\tbuy\tcar\tcars\tengine\tfast\tmiles\tprice\troad\tturn\n",
      "Topic 15:\tbaseball\tgame\tgames\thit\tleague\tplay\tplayers\tseason\tteam\twin\n",
      "Topic 16:\tcard\tcomputer\tdisk\thi\tmac\tmemory\tmonitor\tpc\tsale\tvideo\n",
      "Topic 17:\tchip\tclipper\tencryption\tkey\tkeys\tmessage\tphone\tpublic\tsecret\tsecure\n",
      "Topic 18:\tarticle\tbtw\tdeleted\tdoubt\tfigure\tfolks\thappy\tideas\treading\tsays\n",
      "Topic 19:\tanybody\tappreciated\tgreatly\tguess\thi\tinfo\tnet\tposted\tsorry\twondering\n"
     ]
    }
   ],
   "source": [
    "word = np.argsort(nkw)[:,:-11:-1]\n",
    "for k in range(20):\n",
    "    a = np.zeros((1, X_train.shape[1]))\n",
    "    for w in word[k]:\n",
    "        a[0, w] = 1\n",
    "    print('Topic {}:\\t{}'.format(k, '\\t'.join(vectorizer.inverse_transform(a)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили темы:\n",
    "2. Хоккей\n",
    "3. Религия\n",
    "4. Преступления и их статистика\n",
    "7. Сокращения из Windows\n",
    "8. Разговорная речь\n",
    "9. Космос\n",
    "10. История/Страны\n",
    "15. Баскетболл\n",
    "16. Компьютер\n",
    "17. Криптография\n",
    "\n",
    "В исходном датасете все присутствуют. при увеличении числа итерации и расширении словаря получаем более точные данные. \n",
    "(проблемя в том, что занимает очень много времени)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
